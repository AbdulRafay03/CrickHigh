{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10170222,"sourceType":"datasetVersion","datasetId":6281004},{"sourceId":10171757,"sourceType":"datasetVersion","datasetId":6282209},{"sourceId":10173027,"sourceType":"datasetVersion","datasetId":6283164},{"sourceId":10176661,"sourceType":"datasetVersion","datasetId":6285720},{"sourceId":10842224,"sourceType":"datasetVersion","datasetId":6733322},{"sourceId":10843936,"sourceType":"datasetVersion","datasetId":6734560},{"sourceId":11782652,"sourceType":"datasetVersion","datasetId":7397572},{"sourceId":11784469,"sourceType":"datasetVersion","datasetId":7398910},{"sourceId":11784693,"sourceType":"datasetVersion","datasetId":7399080},{"sourceId":195201,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":166441,"modelId":188763}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Dataset Preparation**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms, models\nfrom PIL import Image\nimport os\nfrom sklearn import preprocessing\n\n# Define a custom Dataset class to load images and labels\nclass CricketDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.image_paths = []\n        self.labels = []\n\n        # Iterate through the classes (Bowl Started, Bowl Starting)\n        for label_idx, class_name in enumerate(['Bowl Started', 'Bowl Starting']):\n            class_folder = os.path.join(root_dir, class_name)\n            for img_name in os.listdir(class_folder):\n                if img_name.endswith(\".jpg\") or img_name.endswith(\".png\"):  # Filter image files\n                    img_path = os.path.join(class_folder, img_name)\n                    self.image_paths.append(img_path)\n                    self.labels.append(label_idx)  # 0 for \"Bowl Started\", 1 for \"Bowl Starting\"\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert('RGB')  # Open image and convert to RGB for consistency\n        label = self.labels[idx]\n\n        # Preprocess image: convert to grayscale and resize to 224x224\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n# Define image transformations\ntransform = transforms.Compose([\n    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale (1 channel)\n    transforms.Resize((224, 224)),                # Resize to 224x224\n    transforms.ToTensor(),                        # Convert to tensor\n    transforms.Normalize(mean=[0.5], std=[0.5])   # Normalize the image\n])\n\n# Load the datasets\ntrain_dataset = CricketDataset(root_dir='/kaggle/input/cricket-bowling/YoloDataset - Copy/train', transform=transform)\nvalid_dataset = CricketDataset(root_dir='/kaggle/input/cricket-bowling/YoloDataset - Copy/valid', transform=transform)\ntest_dataset = CricketDataset(root_dir='/kaggle/input/cricket-bowling/YoloDataset - Copy/test', transform=transform)\n\n# Create DataLoader objects for training, validation, and testing\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# Load the pre-trained EfficientNetB0 model\nmodel = models.efficientnet_b0(pretrained=True)\n\n# Modify the first convolutional layer to accept single-channel (grayscale) input\n# EfficientNet expects 3 channels (RGB), so we modify the first layer\nmodel.features[0] = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n\n# Modify the final classifier layer to have 2 output classes (Bowl Started, Bowl Starting)\nmodel.classifier[1] = nn.Linear(model.classifier[1].in_features, 2)\n\n# Move the model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Set up the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Model Training**","metadata":{}},{"cell_type":"code","source":"# Function for training the model\ndef train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=5):\n    best_valid_loss = float('inf')\n\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        correct_preds = 0\n        total_preds = 0\n\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n            # Accuracy calculation\n            _, predicted = torch.max(outputs, 1)\n            correct_preds += (predicted == labels).sum().item()\n            total_preds += labels.size(0)\n\n        epoch_loss = running_loss / len(train_loader)\n        epoch_accuracy = 100 * correct_preds / total_preds\n\n        # Validation phase\n        model.eval()\n        valid_loss = 0.0\n        correct_preds = 0\n        total_preds = 0\n\n        with torch.no_grad():\n            for inputs, labels in valid_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                valid_loss += loss.item()\n\n                _, predicted = torch.max(outputs, 1)\n                correct_preds += (predicted == labels).sum().item()\n                total_preds += labels.size(0)\n\n        valid_loss = valid_loss / len(valid_loader)\n        valid_accuracy = 100 * correct_preds / total_preds\n\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        print(f\"Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.2f}%\")\n        print(f\"Validation Loss: {valid_loss:.4f}, Validation Accuracy: {valid_accuracy:.2f}%\")\n\n        # Save the best model based on validation loss\n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            torch.save(model.state_dict(), \"best_model.pth\")\n            print(\"Saved best model\")\n\n# Train the model\ntrain_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=30)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Model Evaluation**","metadata":{}},{"cell_type":"code","source":"# Evaluate the model on the test set\ndef evaluate_model(model, test_loader):\n    model.eval()\n    correct_preds = 0\n    total_preds = 0\n\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            outputs = model(inputs)\n\n            _, predicted = torch.max(outputs, 1)\n            correct_preds += (predicted == labels).sum().item()\n            total_preds += labels.size(0)\n\n    test_accuracy = 100 * correct_preds / total_preds\n    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n\n# Evaluate the model on the test set\nevaluate_model(model, test_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torchvision import models\nimport cv2\nimport os\nfrom PIL import Image\n\n# Load the trained EfficientNetB0 model\nmodel_path = \"/kaggle/input/efficentnetb0/pytorch/default/1/best_model (1).pth\"\nmodel = models.efficientnet_b0(pretrained=False)\nmodel.features[0] = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\nmodel.classifier[1] = nn.Linear(model.classifier[1].in_features, 2)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\nmodel.load_state_dict(torch.load(model_path, map_location=device))\nmodel.eval()  # Set the model to evaluation mode\n\n# Define image transformations for inference\ntransform = transforms.Compose([\n    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n    transforms.Resize((224, 224)),                # Resize to 224x224\n    transforms.ToTensor(),                        # Convert to tensor\n    transforms.Normalize(mean=[0.5], std=[0.5])   # Normalize the image\n])\n\n# Function to process video and make predictions\ndef process_video(video_path, model):\n    cap = cv2.VideoCapture(video_path)\n    fps = cap.get(cv2.CAP_PROP_FPS)  # Get frames per second\n    results = []  # List to store predictions with timestamps\n\n    frame_count = 0\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Convert frame to PIL Image\n        frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n        \n        # Apply transformations\n        frame_tensor = transform(frame_pil).unsqueeze(0).to(device)\n\n        # Make prediction\n        with torch.no_grad():\n            output = model(frame_tensor)\n            _, predicted = torch.max(output, 1)\n\n        # Get timestamp in hh:mm:ss format\n        timestamp = frame_count / fps\n        time_hhmmss = f\"{int(timestamp // 3600):02}:{int((timestamp % 3600) // 60):02}:{int(timestamp % 60):02}\"\n\n        # Append prediction and timestamp\n        results.append((predicted.item(), time_hhmmss))\n\n        frame_count += 1\n\n    cap.release()\n    return results\n\n# Path to the video\nvideo_path = \"/kaggle/input/cricket-shaheen-bowling/shaheen.mp4\"\n\n# Process the video and get predictions\npredictions = process_video(video_path, model)\n\n# Print the results\nfor pred, timestamp in predictions:\n    class_name = \"Bowl Started\" if pred == 0 else \"Bowl Starting\"\n    print(f\"Timestamp: {timestamp}, Prediction: {class_name}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torchvision import models\nimport cv2\nfrom PIL import Image\nimport easyocr\nimport re\nfrom collections import deque, Counter\n\n# Load EfficientNetB0\nmodel_path = \"/kaggle/input/efficentnetb0/pytorch/default/1/best_model (1).pth\"\nmodel = models.efficientnet_b0(pretrained=False)\nmodel.features[0] = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\nmodel.classifier[1] = nn.Linear(model.classifier[1].in_features, 2)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\nmodel.load_state_dict(torch.load(model_path, map_location=device))\nmodel.eval()\n\ntransform = transforms.Compose([\n    transforms.Grayscale(num_output_channels=1),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5])\n])\n\ndef extract_cricket_score_from_frame(frame, reader):\n    height, width = frame.shape[:2]\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    y_start = int(0.76 * height)\n    y_end = int(0.986 * height)\n    x_start = 0\n    x_end = int(0.47 * width)\n    region = gray[y_start:y_end, x_start:x_end]\n    _, binary = cv2.threshold(region, 150, 255, cv2.THRESH_BINARY_INV)\n    result = reader.readtext(binary, detail=0)\n    ocr_text = ' '.join(result)\n    match = re.search(r'\\b(\\d+)-(\\d+)\\b', ocr_text)\n    if match:\n        return int(match.group(1)), int(match.group(2))\n    return None, None\n\ndef has_consecutive_repeats(seq, value, min_repeat):\n    repeat = 0\n    for item in seq:\n        if item == value:\n            repeat += 1\n            if repeat >= min_repeat:\n                return True\n        else:\n            repeat = 0\n    return False\n\ndef get_stable_score(score_window):\n    counts = Counter(score_window)\n    for score, count in counts.items():\n        if count >= 3:\n            if has_consecutive_repeats(score_window, score, min_repeat=3) or count == max(counts.values()):\n                return score\n    return None\n\ndef process_video(video_path, frame_skip=10):\n    cap = cv2.VideoCapture(video_path)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    reader = easyocr.Reader(['en'])\n\n    frame_count = 0\n    timestamps_effnet = []\n    timestamps_ocr = []\n    frame_window = []\n    pause_counter = 0\n    last_score = None\n    ocr_score_window = deque(maxlen=15)\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        if pause_counter > 0:\n            pause_counter -= 1\n            frame_count += 1\n            continue\n\n        img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n        img_tensor = transform(img).unsqueeze(0).to(device)\n\n        with torch.no_grad():\n            outputs = model(img_tensor)\n            _, predicted = torch.max(outputs, 1)\n            prediction = predicted.item()\n\n        frame_window.append(prediction)\n        if len(frame_window) > 20:\n            frame_window.pop(0)\n\n        if frame_window.count(1) >= 15:\n            consecutive_count = 0\n            for pred in frame_window:\n                if pred == 1:\n                    consecutive_count += 1\n                    if consecutive_count >= 9:\n                        timestamp_sec = frame_count / fps\n                        minutes = int(timestamp_sec // 60)\n                        seconds = int(timestamp_sec % 60)\n                        hours = int(minutes // 60)\n                        minutes = minutes % 60\n                        timestamp = f\"{hours:02}:{minutes:02}:{seconds:02}\"\n                        timestamps_effnet.append(timestamp)\n                        pause_counter = int(10 * fps)\n                        frame_window.clear()\n                        break\n                else:\n                    consecutive_count = 0\n\n        if frame_count % frame_skip == 0:\n            runs, wickets = extract_cricket_score_from_frame(frame, reader)\n            if runs is not None and wickets is not None:\n                ocr_score_window.append((runs, wickets))\n\n            stable_score = get_stable_score(list(ocr_score_window))\n            if stable_score:\n                confirmed_runs, confirmed_wickets = stable_score\n                if last_score is None:\n                    last_score = (confirmed_runs, confirmed_wickets)\n                    should_add = False\n                else:\n                    run_diff = confirmed_runs - last_score[0]\n                    wicket_diff = confirmed_wickets - last_score[1]\n                    valid_run = run_diff in [4, 6]\n                    valid_wicket = wicket_diff == 1\n                    should_add = valid_run or valid_wicket\n                    last_score = (confirmed_runs, confirmed_wickets)\n    \n                if should_add:\n                    timestamp_sec = frame_count / fps\n                    minutes = int(timestamp_sec // 60)\n                    seconds = int(timestamp_sec % 60)\n                    hours = int(minutes // 60)\n                    minutes = minutes % 60\n                    timestamp = f\"{hours:02}:{minutes:02}:{seconds:02}\"\n                    timestamps_ocr.append((confirmed_runs, confirmed_wickets, last_score, timestamp))\n\n        frame_count += 1\n\n    cap.release()\n    return timestamps_effnet, timestamps_ocr\n\n# Example usage\nvideo_path = \"/kaggle/input/fullt20-pakvnz/Full Lenght Match.mp4\"\ntimestamps_effnet, timestamps_ocr = process_video(video_path)\n\nprint(\"EfficientNet Timestamps:\")\nprint(timestamps_effnet)\n\nprint(\"\\nOCR Timestamps and Scores:\")\nfor runs, wickets, last_score, timestamp in timestamps_ocr:\n    print(f\"Runs: {runs}, Wickets: {wickets}, Last Score: {last_score}, Timestamp: {timestamp}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T17:57:19.130743Z","iopub.execute_input":"2025-05-12T17:57:19.131379Z","iopub.status.idle":"2025-05-12T20:45:30.079431Z","shell.execute_reply.started":"2025-05-12T17:57:19.131341Z","shell.execute_reply":"2025-05-12T20:45:30.078430Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/1960700376.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path, map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"EfficientNet Timestamps:\n['00:02:34', '00:02:45', '00:03:20', '00:04:15', '00:04:48', '00:05:11', '00:05:37', '00:06:49', '00:07:28', '00:08:11', '00:08:53', '00:09:16', '00:09:31', '00:10:12', '00:11:02', '00:11:17', '00:11:49', '00:12:21', '00:12:39', '00:13:00', '00:13:36', '00:14:11', '00:15:13', '00:15:46', '00:16:37', '00:17:30', '00:18:06', '00:18:42', '00:19:59', '00:20:33', '00:20:51', '00:21:02', '00:21:12', '00:22:01', '00:22:12', '00:22:56', '00:23:35', '00:24:09', '00:24:51', '00:25:54', '00:26:29', '00:27:04', '00:27:45', '00:28:16', '00:30:37', '00:31:11', '00:31:23', '00:31:43', '00:32:26', '00:32:38', '00:33:05', '00:33:37', '00:34:10', '00:34:30', '00:36:23', '00:36:59', '00:37:20', '00:37:30', '00:37:59', '00:38:21', '00:38:35', '00:38:45', '00:39:47', '00:40:14', '00:40:31', '00:41:02', '00:41:35', '00:42:04', '00:42:56', '00:43:11', '00:43:28', '00:43:41', '00:44:05', '00:46:03', '00:46:48', '00:47:00', '00:47:53', '00:48:14', '00:48:31', '00:49:01', '00:49:20', '00:49:43', '00:50:04', '00:51:03', '00:51:27', '00:51:46', '00:52:09', '00:52:23', '00:52:48', '00:53:18', '00:54:05', '00:54:30', '00:54:53', '00:55:22', '00:57:21', '00:57:55', '00:58:16', '00:59:20', '01:00:04', '01:00:27', '01:01:07', '01:01:50', '01:02:19', '01:02:30', '01:03:48', '01:04:12', '01:04:35', '01:05:17', '01:05:51', '01:06:02', '01:06:46', '01:07:32', '01:08:46', '01:09:24', '01:09:51', '01:10:09', '01:10:35', '01:11:08', '01:12:19', '01:12:30', '01:12:43', '01:12:58', '01:13:31', '01:13:59', '01:14:20', '01:14:38', '01:15:16', '01:16:08', '01:16:22', '01:18:14', '01:18:49', '01:19:35', '01:20:07', '01:20:25', '01:21:05', '01:22:20', '01:23:01', '01:24:02', '01:24:39', '01:25:19', '01:25:56', '01:27:14', '01:27:53', '01:29:16', '01:29:58', '01:31:25', '01:32:04', '01:33:22', '01:33:54', '01:34:24', '01:34:38', '01:34:54', '01:35:05', '01:35:24', '01:36:53', '01:37:28', '01:38:11', '01:38:37', '01:39:25', '01:40:04', '01:40:21', '01:41:05', '01:41:38', '01:42:25', '01:43:00', '01:43:36', '01:44:18', '01:45:06', '01:45:37', '01:46:00', '01:46:46', '01:47:19', '01:47:52', '01:48:45', '01:49:28', '01:50:12', '01:50:49', '01:52:46', '01:53:24', '01:54:29', '01:55:00', '01:55:15', '01:56:02', '01:56:23', '01:56:37', '01:57:02', '01:58:02', '02:00:57', '02:01:07', '02:01:28', '02:01:48', '02:02:02', '02:02:13', '02:02:49', '02:03:00', '02:03:15', '02:04:16', '02:04:46', '02:05:13', '02:05:36', '02:05:54', '02:06:22', '02:06:51', '02:07:40', '02:08:05', '02:08:29', '02:08:50', '02:09:16', '02:09:35', '02:09:54', '02:10:48', '02:10:59', '02:11:24', '02:12:16', '02:12:42', '02:13:01', '02:13:22', '02:13:58', '02:15:20', '02:15:42', '02:16:13', '02:16:37', '02:17:04', '02:17:33', '02:17:46', '02:19:29', '02:20:04', '02:20:15', '02:20:39', '02:23:15', '02:23:52', '02:24:29', '02:26:36', '02:27:14', '02:27:48', '02:28:04', '02:28:29', '02:29:37', '02:29:59', '02:30:41', '02:31:07', '02:31:24', '02:31:58', '02:33:06', '02:33:31', '02:34:04', '02:34:17', '02:34:38', '02:35:29', '02:37:27', '02:38:03', '02:38:36', '02:38:46', '02:39:20', '02:40:18', '02:40:47', '02:41:56', '02:42:08', '02:42:19', '02:42:36', '02:42:53', '02:43:18', '02:43:54', '02:44:20', '02:44:40', '02:46:02', '02:46:13', '02:46:57', '02:47:44', '02:48:28', '02:49:06', '02:49:37', '02:49:47', '02:50:34', '02:51:12', '02:53:15', '02:54:00', '02:55:21', '02:56:17', '02:57:37']\n\nOCR Timestamps and Scores:\nRuns: 6, Wickets: 0, Last Score: (6, 0), Timestamp: 00:03:18\nRuns: 17, Wickets: 0, Last Score: (17, 0), Timestamp: 00:07:43\nRuns: 21, Wickets: 0, Last Score: (21, 0), Timestamp: 00:08:25\nRuns: 28, Wickets: 0, Last Score: (28, 0), Timestamp: 00:13:51\nRuns: 35, Wickets: 0, Last Score: (35, 0), Timestamp: 00:16:00\nRuns: 46, Wickets: 0, Last Score: (46, 0), Timestamp: 00:24:23\nRuns: 50, Wickets: 1, Last Score: (50, 1), Timestamp: 00:28:36\nRuns: 56, Wickets: 2, Last Score: (56, 2), Timestamp: 00:35:26\nRuns: 67, Wickets: 3, Last Score: (67, 3), Timestamp: 00:44:27\nRuns: 82, Wickets: 3, Last Score: (82, 3), Timestamp: 00:55:07\nRuns: 83, Wickets: 4, Last Score: (83, 4), Timestamp: 00:56:12\nRuns: 97, Wickets: 4, Last Score: (97, 4), Timestamp: 01:06:18\nRuns: 101, Wickets: 4, Last Score: (101, 4), Timestamp: 01:07:46\nRuns: 105, Wickets: 4, Last Score: (105, 4), Timestamp: 01:09:01\nRuns: 117, Wickets: 4, Last Score: (117, 4), Timestamp: 01:14:13\nRuns: 121, Wickets: 4, Last Score: (121, 4), Timestamp: 01:14:52\nRuns: 125, Wickets: 4, Last Score: (125, 4), Timestamp: 01:15:31\nRuns: 125, Wickets: 5, Last Score: (125, 5), Timestamp: 01:16:42\nRuns: 131, Wickets: 5, Last Score: (131, 5), Timestamp: 01:23:15\nRuns: 138, Wickets: 5, Last Score: (138, 5), Timestamp: 01:24:53\nRuns: 148, Wickets: 5, Last Score: (148, 5), Timestamp: 01:27:28\nRuns: 148, Wickets: 6, Last Score: (148, 6), Timestamp: 01:28:13\nRuns: 149, Wickets: 7, Last Score: (149, 7), Timestamp: 01:30:41\nRuns: 4, Wickets: 0, Last Score: (4, 0), Timestamp: 01:33:53\nRuns: 8, Wickets: 0, Last Score: (8, 0), Timestamp: 01:34:08\nRuns: 16, Wickets: 0, Last Score: (16, 0), Timestamp: 01:37:49\nRuns: 21, Wickets: 0, Last Score: (21, 0), Timestamp: 01:39:42\nRuns: 28, Wickets: 0, Last Score: (28, 0), Timestamp: 01:41:55\nRuns: 39, Wickets: 0, Last Score: (39, 0), Timestamp: 01:49:05\nRuns: 40, Wickets: 1, Last Score: (40, 1), Timestamp: 01:51:11\nRuns: 46, Wickets: 1, Last Score: (46, 1), Timestamp: 01:54:43\nRuns: 65, Wickets: 1, Last Score: (65, 1), Timestamp: 02:04:35\nRuns: 81, Wickets: 1, Last Score: (81, 1), Timestamp: 02:13:40\nRuns: 95, Wickets: 1, Last Score: (95, 1), Timestamp: 02:20:53\nRuns: 96, Wickets: 2, Last Score: (96, 2), Timestamp: 02:24:47\nRuns: 114, Wickets: 2, Last Score: (114, 2), Timestamp: 02:34:57\nRuns: 114, Wickets: 3, Last Score: (114, 3), Timestamp: 02:35:56\nRuns: 120, Wickets: 3, Last Score: (120, 3), Timestamp: 02:37:41\nRuns: 128, Wickets: 3, Last Score: (128, 3), Timestamp: 02:39:01\nRuns: 138, Wickets: 3, Last Score: (138, 3), Timestamp: 02:43:32\nRuns: 148, Wickets: 4, Last Score: (148, 4), Timestamp: 02:54:19\nRuns: 154, Wickets: 4, Last Score: (154, 4), Timestamp: 02:56:32\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# import torchvision.transforms as transforms\n# from torchvision import models\n# import cv2\n# from PIL import Image\n# import easyocr\n# import re\n# from collections import deque, Counter\n\n\n# def extract_cricket_score_from_frame(frame, reader):\n#     height, width = frame.shape[:2]\n#     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n#     y_start = int(0.76 * height)\n#     y_end = int(0.986 * height)\n#     x_start = 0\n#     x_end = int(0.47 * width)\n#     region = gray[y_start:y_end, x_start:x_end]\n#     _, binary = cv2.threshold(region, 150, 255, cv2.THRESH_BINARY_INV)\n#     result = reader.readtext(binary, detail=0)\n#     ocr_text = ' '.join(result)\n#     match = re.search(r'\\b(\\d+)-(\\d+)\\b', ocr_text)\n#     if match:\n#         return int(match.group(1)), int(match.group(2))\n#     return None, None\n\n# def has_consecutive_repeats(seq, value, min_repeat):\n#     repeat = 0\n#     for item in seq:\n#         if item == value:\n#             repeat += 1\n#             if repeat >= min_repeat:\n#                 return True\n#         else:\n#             repeat = 0\n#     return False\n\n# def get_stable_score(score_window):\n#     counts = Counter(score_window)\n#     for score, count in counts.items():\n#         if count >= 2:\n#             if has_consecutive_repeats(score_window, score, min_repeat=3) or count == max(counts.values()):\n#                 return score\n#     return None\n# def process_video(video_path, frame_skip=15, start_time_sec=0):\n#     cap = cv2.VideoCapture(video_path)\n#     reader = easyocr.Reader(['en'])\n#     fps = cap.get(cv2.CAP_PROP_FPS)\n#     frame_count = 0\n#     last_score = None\n#     score_window = deque(maxlen=15)\n\n#     # Skip to the starting frame (26 seconds)\n#     start_frame = int(start_time_sec * fps)\n#     cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n#     frame_count = start_frame\n\n#     while cap.isOpened():\n#         ret, frame = cap.read()\n#         if not ret:\n#             break\n\n#         runs, wickets = extract_cricket_score_from_frame(frame, reader)\n#         if runs is not None and wickets is not None:\n#             score_window.append((runs, wickets))\n\n#         stable_score = get_stable_score(list(score_window))\n\n#         timestamp_sec = frame_count / fps\n#         minutes = int(timestamp_sec // 60)\n#         seconds = int(timestamp_sec % 60)\n#         hours = int(minutes // 60)\n#         minutes = minutes % 60\n#         timestamp = f\"{hours:02}:{minutes:02}:{seconds:02}\"\n#         wicket_diff = 0\n#         run_diff = 0\n#         if stable_score:\n#             confirmed_runs, confirmed_wickets = stable_score\n#             if last_score is None:\n#                 last_score = (confirmed_runs, confirmed_wickets)\n#                 should_add = False\n#             else:\n#                 run_diff = confirmed_runs - last_score[0]\n#                 wicket_diff = confirmed_wickets - last_score[1]\n#                 valid_run = run_diff in [4, 6]\n#                 valid_wicket = wicket_diff == 1\n#                 should_add = valid_run or valid_wicket\n#                 last_score = (confirmed_runs, confirmed_wickets)\n#             if should_add:\n#                 print(f\"✅ Time {timestamp}: Score = {confirmed_runs}-{confirmed_wickets} , Last Score: {last_score} , Wicket: {wicket_diff} , run_diff {run_diff} (Highlight)\")\n#             else:\n#                 print(f\"❌ Time {timestamp}: Score = {confirmed_runs}-{confirmed_wickets}, Last Score: {last_score} , Wicket: {wicket_diff} , run_diff {run_diff} (Not a highlight)\")\n        \n#         else:\n#             print(f\"⏳ Time {timestamp}: Score unstable or not detected\")\n\n#         for _ in range(frame_skip - 1):\n#             cap.read()\n#             frame_count += 1\n\n#         frame_count += 1\n\n#     cap.release()\n\n\n# # Example usage\n# video_path = \"/kaggle/input/50-se-56-wali-video/50 se 56 wali video - Made with Clipchamp (1).mp4\"\n# process_video(video_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\n\nfile = \"/kaggle/input/30minvideo/Untitled video - Made with Clipchamp (1).mp4\"\n\ncap = cv2.VideoCapture(file)\n\nif not cap.isOpened():\n    print(\"Error opening video file\")\nelse:\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    print(f\"Frame size: {width} x {height}\")\n\ncap.release()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T20:47:31.642965Z","iopub.execute_input":"2025-05-12T20:47:31.643685Z","iopub.status.idle":"2025-05-12T20:47:31.715402Z","shell.execute_reply.started":"2025-05-12T20:47:31.643644Z","shell.execute_reply":"2025-05-12T20:47:31.714420Z"}},"outputs":[{"name":"stdout","text":"Frame size: 852 x 480\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# EfficientNet Timestamps:\n# ['00:00:00', '00:00:36', '00:01:09', '00:01:52', '00:02:55', '00:03:30', '00:04:05', '00:04:45', '00:05:16', '00:07:37', '00:08:11', '00:08:23', '00:08:43', '00:09:26', '00:09:39', '00:10:05', '00:10:37', '00:11:10', '00:11:30', '00:13:23', '00:13:59', '00:14:20', '00:14:31', '00:14:59', '00:15:21', '00:15:35', '00:15:46', '00:16:47', '00:17:14', '00:17:31', '00:18:02', '00:18:36', '00:19:05', '00:19:56', '00:20:11', '00:20:28', '00:20:41', '00:21:06', '00:23:04', '00:23:49', '00:24:01', '00:24:54', '00:25:14', '00:25:32', '00:26:01', '00:26:20', '00:26:43', '00:27:04', '00:28:03', '00:28:27', '00:28:47', '00:29:10', '00:29:24', '00:29:49']\n\n# OCR Timestamps and Scores:\n# Runs: 41, Wickets: 0, Timestamp: 00:00:11\n# Runs: 46, Wickets: 0, Timestamp: 00:01:20\n# Runs: 50, Wickets: 0, Timestamp: 00:04:16\n# Runs: 50, Wickets: 1, Timestamp: 00:05:33\n# Runs: 55, Wickets: 1, Timestamp: 00:10:55\n# Runs: 56, Wickets: 2, Timestamp: 00:12:23\n# Runs: 60, Wickets: 2, Timestamp: 00:14:43\n# Runs: 65, Wickets: 2, Timestamp: 00:18:48\n# Runs: 67, Wickets: 3, Timestamp: 00:21:24\n# Runs: 72, Wickets: 3, Timestamp: 00:27:19\n# Runs: 76, Wickets: 3, Timestamp: 00:29:35","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T20:47:32.135413Z","iopub.execute_input":"2025-05-12T20:47:32.135802Z","iopub.status.idle":"2025-05-12T20:47:32.140209Z","shell.execute_reply.started":"2025-05-12T20:47:32.135769Z","shell.execute_reply":"2025-05-12T20:47:32.139326Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"print(\"\\nOCR Timestamps and Scores:\")\nprint(len(timestamps_ocr))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T20:47:35.326064Z","iopub.execute_input":"2025-05-12T20:47:35.326675Z","iopub.status.idle":"2025-05-12T20:47:35.331158Z","shell.execute_reply.started":"2025-05-12T20:47:35.326640Z","shell.execute_reply":"2025-05-12T20:47:35.330083Z"}},"outputs":[{"name":"stdout","text":"\nOCR Timestamps and Scores:\n42\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"# **Concatenating TimeStamps **","metadata":{}},{"cell_type":"code","source":"from datetime import datetime\nfrom datetime import datetime, timedelta\ndef find_closest_timestamps(ocr_list, effnet_list):\n\n    # Convert EfficientNet timestamps to datetime objects for easy comparison\n    effnet_times = [datetime.strptime(ts, \"%H:%M:%S\") for ts in effnet_list]\n\n    # Result list to store combined values\n    result = []\n\n    # Loop through each value in the OCR list\n    for runs, wickets,_, ocr_timestamp in ocr_list:\n        ocr_time = datetime.strptime(ocr_timestamp, \"%H:%M:%S\")\n    \n        # Filter only those EfficientNet times that are <= OCR time\n        valid_effnet_times = [eff_time for eff_time in effnet_times if eff_time <= ocr_time]\n    \n        if not valid_effnet_times:\n            continue  # skip if no valid timestamp before OCR time\n    \n        # Now get the closest from valid ones (latest before OCR)\n        closest_effnet_time = max(valid_effnet_times, key=lambda eff_time: eff_time)\n    \n        closest_effnet_str = closest_effnet_time.strftime(\"%H:%M:%S\")\n        result.append((closest_effnet_str, ocr_timestamp))\n\n\n    return result\ndef add_seconds_to_time(time_str, seconds_to_add):\n    # Convert the time string to a datetime object\n    time_obj = datetime.strptime(time_str, '%H:%M:%S')\n    \n    # Add the specified seconds using timedelta\n    new_time_obj = time_obj + timedelta(seconds=seconds_to_add)\n    \n    # Convert it back to a time string\n    return new_time_obj.strftime('%H:%M:%S')\n\n\n\nnew_list = find_closest_timestamps(timestamps_ocr, timestamps_effnet)\n# print(new_list)\n\nnew_list = [(start, add_seconds_to_time(end, 5)) for start, end in new_list]\n# Print the result\nprint(new_list)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T20:47:36.704503Z","iopub.execute_input":"2025-05-12T20:47:36.704836Z","iopub.status.idle":"2025-05-12T20:47:36.718546Z","shell.execute_reply.started":"2025-05-12T20:47:36.704807Z","shell.execute_reply":"2025-05-12T20:47:36.717597Z"}},"outputs":[{"name":"stdout","text":"[('00:02:45', '00:03:23'), ('00:07:28', '00:07:48'), ('00:08:11', '00:08:30'), ('00:13:36', '00:13:56'), ('00:15:46', '00:16:05'), ('00:24:09', '00:24:28'), ('00:28:16', '00:28:41'), ('00:34:30', '00:35:31'), ('00:44:05', '00:44:32'), ('00:54:53', '00:55:12'), ('00:55:22', '00:56:17'), ('01:06:02', '01:06:23'), ('01:07:32', '01:07:51'), ('01:08:46', '01:09:06'), ('01:13:59', '01:14:18'), ('01:14:38', '01:14:57'), ('01:15:16', '01:15:36'), ('01:16:22', '01:16:47'), ('01:23:01', '01:23:20'), ('01:24:39', '01:24:58'), ('01:27:14', '01:27:33'), ('01:27:53', '01:28:18'), ('01:29:58', '01:30:46'), ('01:33:22', '01:33:58'), ('01:33:54', '01:34:13'), ('01:37:28', '01:37:54'), ('01:39:25', '01:39:47'), ('01:41:38', '01:42:00'), ('01:48:45', '01:49:10'), ('01:50:49', '01:51:16'), ('01:54:29', '01:54:48'), ('02:04:16', '02:04:40'), ('02:13:22', '02:13:45'), ('02:20:39', '02:20:58'), ('02:24:29', '02:24:52'), ('02:34:38', '02:35:02'), ('02:35:29', '02:36:01'), ('02:37:27', '02:37:46'), ('02:38:46', '02:39:06'), ('02:43:18', '02:43:37'), ('02:54:00', '02:54:24'), ('02:56:17', '02:56:37')]\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"# **Clips Extraction**","metadata":{}},{"cell_type":"code","source":"import os\nimport subprocess\n\ndef extract_clips_with_ffmpeg(video_path, clip_timestamps, output_folder=\"/kaggle/working/new2\"):\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n\n    for idx, (start, end) in enumerate(clip_timestamps):\n        # Create output file name\n        output_file = os.path.join(output_folder, f\"clip2_{idx + 1}.mp4\")\n        \n        # Prepare ffmpeg command\n        cmd = [\n            'ffmpeg', \n            '-i', video_path, \n            '-ss', start,  # Start time\n            '-to', end,    # End time\n            '-c', 'copy',  # Copy video codec\n            output_file    # Output file path\n        ]\n        \n        # Run the command\n        subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n        print(f\"Extracted clip {idx + 1}: {start} to {end}\")\n\n# Example usage\nvideo_path = \"/kaggle/input/fullt20-pakvnz/Full Lenght Match.mp4\"\nextract_clips_with_ffmpeg(video_path, new_list)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T20:50:13.440908Z","iopub.execute_input":"2025-05-12T20:50:13.441694Z","iopub.status.idle":"2025-05-12T20:50:19.507661Z","shell.execute_reply.started":"2025-05-12T20:50:13.441646Z","shell.execute_reply":"2025-05-12T20:50:19.506606Z"}},"outputs":[{"name":"stdout","text":"Extracted clip 1: 00:02:45 to 00:03:23\nExtracted clip 2: 00:07:28 to 00:07:48\nExtracted clip 3: 00:08:11 to 00:08:30\nExtracted clip 4: 00:13:36 to 00:13:56\nExtracted clip 5: 00:15:46 to 00:16:05\nExtracted clip 6: 00:24:09 to 00:24:28\nExtracted clip 7: 00:28:16 to 00:28:41\nExtracted clip 8: 00:34:30 to 00:35:31\nExtracted clip 9: 00:44:05 to 00:44:32\nExtracted clip 10: 00:54:53 to 00:55:12\nExtracted clip 11: 00:55:22 to 00:56:17\nExtracted clip 12: 01:06:02 to 01:06:23\nExtracted clip 13: 01:07:32 to 01:07:51\nExtracted clip 14: 01:08:46 to 01:09:06\nExtracted clip 15: 01:13:59 to 01:14:18\nExtracted clip 16: 01:14:38 to 01:14:57\nExtracted clip 17: 01:15:16 to 01:15:36\nExtracted clip 18: 01:16:22 to 01:16:47\nExtracted clip 19: 01:23:01 to 01:23:20\nExtracted clip 20: 01:24:39 to 01:24:58\nExtracted clip 21: 01:27:14 to 01:27:33\nExtracted clip 22: 01:27:53 to 01:28:18\nExtracted clip 23: 01:29:58 to 01:30:46\nExtracted clip 24: 01:33:22 to 01:33:58\nExtracted clip 25: 01:33:54 to 01:34:13\nExtracted clip 26: 01:37:28 to 01:37:54\nExtracted clip 27: 01:39:25 to 01:39:47\nExtracted clip 28: 01:41:38 to 01:42:00\nExtracted clip 29: 01:48:45 to 01:49:10\nExtracted clip 30: 01:50:49 to 01:51:16\nExtracted clip 31: 01:54:29 to 01:54:48\nExtracted clip 32: 02:04:16 to 02:04:40\nExtracted clip 33: 02:13:22 to 02:13:45\nExtracted clip 34: 02:20:39 to 02:20:58\nExtracted clip 35: 02:24:29 to 02:24:52\nExtracted clip 36: 02:34:38 to 02:35:02\nExtracted clip 37: 02:35:29 to 02:36:01\nExtracted clip 38: 02:37:27 to 02:37:46\nExtracted clip 39: 02:38:46 to 02:39:06\nExtracted clip 40: 02:43:18 to 02:43:37\nExtracted clip 41: 02:54:00 to 02:54:24\nExtracted clip 42: 02:56:17 to 02:56:37\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"# Clips Concatenation","metadata":{}},{"cell_type":"code","source":"import os\nimport subprocess\n\ndef concatenate_clips(clip_folder, output_file):\n    # Get the list of all extracted clips\n    clips = [os.path.join(clip_folder, f) for f in os.listdir(clip_folder) if f.endswith('.mp4')]\n    \n    # Sort clips by name (ensure the order is correct)\n    clips.sort()\n\n    # Prepare a file list for ffmpeg to concatenate\n    file_list_path = '/kaggle/working/new2/clip_list.txt'\n    with open(file_list_path, 'w') as f:\n        for clip in clips:\n            f.write(f\"file '{clip}'\\n\")\n    \n    # Concatenate clips using ffmpeg\n    cmd = [\n        'ffmpeg',\n        '-f', 'concat',\n        '-safe', '0',\n        '-i', file_list_path,\n        '-c', 'copy',  # Copy video codec\n        output_file     # Output concatenated video file\n    ]\n    \n    subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    print(f\"Concatenated video saved to {output_file}\")\n\n# Example usage\nclip_folder = \"/kaggle/working/new2/\"\noutput_file = \"/kaggle/working/new2/FullMatch2.mp4\"\nconcatenate_clips(clip_folder, output_file)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T20:51:44.934893Z","iopub.execute_input":"2025-05-12T20:51:44.935593Z","iopub.status.idle":"2025-05-12T20:51:45.084865Z","shell.execute_reply.started":"2025-05-12T20:51:44.935556Z","shell.execute_reply":"2025-05-12T20:51:45.083845Z"}},"outputs":[{"name":"stdout","text":"Concatenated video saved to /kaggle/working/new2/FullMatch2.mp4\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}